{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b46ab38-93a2-4df2-a3c6-8e135fceab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Theory and ConceptsUSr Explain the concept of batch normalization in the context of Artificial Neural NetworksrEr Describe the benefits of using batch normalization during trainingr@r Discuss the working principle of batch normalization, including the normalization step and the learnableparameters.\n",
    "\n",
    "\n",
    "## Batch Normalization in Artificial Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "#Batch normalization is a technique used in deep learning to normalize the inputs of each layer, improving the stability and speed of training.\n",
    "\n",
    "#Benefits of Batch Normalization\n",
    "\n",
    "#1. Reduced Internal Covariate Shift: Normalizes the distribution of inputs to each layer, reducing the impact of changing input distributions.\n",
    "#2. Improved Gradient Flow: Enhances gradient flow, reducing vanishing or exploding gradients.\n",
    "#3. Faster Training: Speeds up training by reducing the number of iterations.\n",
    "#4. Regularization: Acts as a regularization technique, reducing overfitting.\n",
    "#5. Improved Generalization: Enhances model generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa41424-a070-4760-9fa3-677ffab371c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformation for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the simple feedforward neural network without batch normalization\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3)  # Flatten the image\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e7266d-92e7-4c70-8000-48758cf1ed7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m large_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# DataLoader with smaller batch size\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m trainloader_small_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39msmall_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m testloader_small_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(testset, batch_size\u001b[38;5;241m=\u001b[39msmall_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# DataLoader with larger batch size\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Q3. Experimentation and AnalysisUSr Experiment with different batch sizes and observe the effect on the training dynamics and modelperformancerEr Discuss the advantages and potential limitations of batch normalization in improving the training ofneural networks.\n",
    "# Modify batch size\n",
    "small_batch_size = 32\n",
    "large_batch_size = 128\n",
    "\n",
    "# DataLoader with smaller batch size\n",
    "trainloader_small_batch = torch.utils.data.DataLoader(trainset, batch_size=small_batch_size, shuffle=True)\n",
    "testloader_small_batch = torch.utils.data.DataLoader(testset, batch_size=small_batch_size, shuffle=False)\n",
    "\n",
    "# DataLoader with larger batch size\n",
    "trainloader_large_batch = torch.utils.data.DataLoader(trainset, batch_size=large_batch_size, shuffle=True)\n",
    "testloader_large_batch = torch.utils.data.DataLoader(testset, batch_size=large_batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "149eba09-3eb7-454c-8315-7ca73ccdf4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with small batch size (32):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_bn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model with small batch size\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with small batch size (32):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m train(\u001b[43mmodel_bn\u001b[49m, trainloader_small_batch)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model with large batch size\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining with large batch size (128):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_bn' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model with small batch size\n",
    "print(\"Training with small batch size (32):\")\n",
    "train(model_bn, trainloader_small_batch)\n",
    "\n",
    "# Train the model with large batch size\n",
    "print(\"\\nTraining with large batch size (128):\")\n",
    "train(model_bn, trainloader_large_batch)\n",
    "\n",
    "# Evaluate both\n",
    "print(\"\\nPerformance with small batch size (32):\")\n",
    "evaluate(model_bn, testloader_small_batch)\n",
    "\n",
    "print(\"\\nPerformance with large batch size (128):\")\n",
    "evaluate(model_bn, testloader_large_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bdd1f-849c-4d8e-bc7a-ee7cc993ed98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
